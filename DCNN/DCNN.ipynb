{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from os.path import basename, join\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../Datasets/NASA C-MAPSS/'\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.txt') and not f.startswith('r')]\n",
    "dataframe = [os.path.splitext(f)[0] for f in text_files]\n",
    "sensor_columns = [\"sensor {}\".format(s) for s in range(1, 22)]\n",
    "info_columns = ['dataset_id', 'unit_id', 'cycle', 'setting 1', 'setting 2', 'setting 3']\n",
    "label_columns = ['dataset_id', 'unit_id', 'rul']\n",
    "settings = ['setting 1', 'setting 2', 'setting 3']\n",
    "\n",
    "\n",
    "test_data = []\n",
    "train_data = []\n",
    "RUL_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RUL(dataframe, Lifetime):\n",
    "    return Lifetime.loc[(dataframe['dataset_id'], dataframe['unit_id'])] - dataframe['cycle']\n",
    "def RUL_by_parts(df, RUL=130):\n",
    "    if df['RUL'] > RUL: return RUL\n",
    "    if df['RUL'] <= RUL: return df['RUL']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in text_files:\n",
    "    print(file)\n",
    "\n",
    "    if re.match('RUL*', file):\n",
    "        subset_df = pd.read_csv(path + file, delimiter=r\"\\s+\", header=None)\n",
    "        unit_id = range(1, subset_df.shape[0] + 1)\n",
    "        subset_df.insert(0, 'unit_id', unit_id)\n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)\n",
    "        RUL_data.append(subset_df)\n",
    "\n",
    "    if re.match('test*', file):\n",
    "        subset_df = pd.read_csv(path + file, delimiter=r\"\\s+\", header=None, usecols=range(26))\n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)\n",
    "        test_data.append(subset_df)\n",
    "\n",
    "    if re.match('train*', file):\n",
    "        subset_df = pd.read_csv(path + file, delimiter=r\"\\s+\", header=None, usecols=range(26))\n",
    "        dataset_id = basename(file).split(\"_\")[1][:5]\n",
    "        subset_df.insert(0, 'dataset_id', dataset_id)\n",
    "        train_data.append(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat(train_data, ignore_index=True)\n",
    "df_train.columns = info_columns + sensor_columns\n",
    "df_train.sort_values(by=['dataset_id', 'unit_id', 'cycle'], inplace=True)\n",
    "\n",
    "df_test = pd.concat(test_data, ignore_index=True)\n",
    "df_test.columns = info_columns + sensor_columns\n",
    "df_test.sort_values(by=['dataset_id', 'unit_id', 'cycle'], inplace=True)\n",
    "\n",
    "df_RUL = pd.concat(RUL_data, ignore_index=True)\n",
    "df_RUL.columns = label_columns\n",
    "df_RUL.sort_values(by=['dataset_id', 'unit_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUL_train = df_train.groupby(['dataset_id', 'unit_id'])['cycle'].max()\n",
    "\n",
    "RUL_test = df_test.groupby(['dataset_id', 'unit_id'])['cycle'].max() + df_RUL.groupby(['dataset_id', 'unit_id'])[\n",
    "'rul'].max()\n",
    "\n",
    "df_train['RUL'] = df_train.apply(lambda r: get_RUL(r, RUL_train), axis=1)\n",
    "df_train['RUL'] = df_train.apply(lambda r: RUL_by_parts(r, 130), axis=1)\n",
    "\n",
    "df_test['RUL'] = df_test.apply(lambda r: get_RUL(r, RUL_test), axis=1)\n",
    "df_test['RUL'] = df_test.apply(lambda r: RUL_by_parts(r, 130), axis=1)\n",
    "\n",
    "df_train.set_index(['dataset_id', 'unit_id'], inplace=True)\n",
    "df_test.set_index(['dataset_id', 'unit_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove constant sensors and settings and apply minmax scaler\n",
    "trainsets = dict.fromkeys([\"FD001\",\"FD002\",\"FD003\",\"FD004\"])\n",
    "testsets= dict.fromkeys([\"FD001\",\"FD002\",\"FD003\",\"FD004\"])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "for i in range(4):\n",
    "    trainsets[\"FD00\"+str(i+1)] = df_train.loc[\"FD00\"+str(i+1)][[\"cycle\",\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]]\n",
    "    trainsets[\"FD00\"+str(i+1)][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\"]]=scaler.fit_transform(df_train.loc[\"FD00\"+str(i+1)][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\"]])\n",
    "    testsets[\"FD00\"+str(i+1)] = df_test.loc[\"FD00\"+str(i+1)][[\"cycle\",\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]]\n",
    "    testsets[\"FD00\"+str(i+1)][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\"]]=scaler.fit_transform(testsets[\"FD00\"+str(i+1)][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(trainsets, open(\"trainsets.p\",\"wb\"))\n",
    "pkl.dump(testsets, open(\"testsets.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rwindows(a, window): \n",
    "    # retourne une fenáº¿tre temporelle de taille window d'un tableau a\n",
    "    if a.ndim == 1:\n",
    "        a = a.reshape(-1, 1)\n",
    "    shape = a.shape[0] - window + 1, window, a.shape[-1]\n",
    "    strides = (a.strides[0],) + a.strides\n",
    "    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "    return np.squeeze(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Nasa(torch.utils.data.Dataset):\n",
    "  def __init__(self, dataset, transform=None, train = True, window = 30):\n",
    "        #Initialization\n",
    "        df = trainsets[\"FD00\"+str(dataset)] if train else testsets[\"FD00\"+str(dataset)]\n",
    "        #remove lines whose number of cycles is below the window size\n",
    "        df = df.drop(list(set(df[df[\"cycle\"] > window].index.tolist()) ^ set(range(1,df.index.tolist()[-1]+1))), axis = 0)\n",
    "        if not train:\n",
    "            # take only the temporal window on last cycles for test datapoints\n",
    "            df_last = [df.loc[i].set_index('cycle').iloc[(-window-1):-1] for i in list(set(df.index.tolist()))]\n",
    "            df_window = np.vstack([rwindows(a[[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]].values,window) for a in df_last]).reshape((-1,window,15))\n",
    "        else:\n",
    "            df_window = np.vstack([rwindows(df.loc[i][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]].values,window) for i in list(set(df.index))])\n",
    "        \n",
    "        #remove lines whose cycle time is less than window-size\n",
    "        \n",
    "        \n",
    "        X = df_window[:,:,:14]\n",
    "        Y = df_window[:,29,14]\n",
    "        \n",
    "        \n",
    "        self.X = X.reshape((X.shape[0],1,window,14))\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        #print(self.Y[index])\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, window_size =9, dropout_ratio = 0.5, initialization='xavier', hidden_neurons=100, activation = 'tanh'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #self.activ = torch.relu if activation == \"relu\" else torch.tanh\n",
    "        init = nn.init.kaiming_normal_ if initialization==\"kaiming\" else nn.init.xavier_normal_\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        init(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10,10,kernel_size=(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        init(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(10,10,(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        init(self.conv3.weight)\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(10,10,(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        init(self.conv4.weight)\n",
    "        nn.init.constant_(self.conv4.bias, 0)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(10,1,(3,1),padding = (1,0))\n",
    "        self.tanh5 = nn.Tanh()\n",
    "        init(self.conv5.weight)\n",
    "        nn.init.constant_(self.conv5.bias, 0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 420, out_features = hidden_neurons)\n",
    "        self.tanh_fc1 = nn.Tanh()\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = hidden_neurons, out_features = 1)\n",
    "        #self.tanh_fc2 = nn.Tanh()\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        tanh1 = self.tanh1(self.conv1(x))\n",
    "        tanh2 = self.tanh2(self.conv2(tanh1))\n",
    "        tanh3 = self.tanh3(self.conv3(tanh2))\n",
    "        tanh4 = self.tanh4(self.conv4(tanh3))\n",
    "        tanh5 = self.tanh5(self.conv5(tanh4))\n",
    "        dropout1 = self.dropout(tanh5)\n",
    "        x_reshape = dropout1.view(-1,30*14)\n",
    "        tanh6 = self.tanh_fc1(self.fc1(x_reshape))\n",
    "        predict = self.fc2(tanh6)\n",
    "        #print(predict)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model, device = torch.device('cpu'), print_graph = False):\n",
    "    MSE = nn.MSELoss()\n",
    "    dtype = torch.float32\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for t, (x,y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            score = model(x)\n",
    "            loss += MSE(score,y.to(device=device, dtype=dtype))\n",
    "        loss = np.sqrt(loss/(t+1))\n",
    "        print('RMSE : (%.2f)' % (loss))\n",
    "        if print_graph:\n",
    "            plt.plot(np.array(score))\n",
    "            plt.plot(np.array(y))\n",
    "            plt.show()\n",
    "        #print('score : (%.2f) , target : (%.2f)' %(score, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer,loader_train, loader_val = None, epochs = 1,USE_GPU = True,print_every = 10):\n",
    "    \n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model = model.to(device = device)\n",
    "    hold_loss = []\n",
    "    RMSE = RMSELoss()\n",
    "    MSE = nn.MSELoss()\n",
    "    \n",
    "    dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "    \n",
    "    weight1 = []\n",
    "    weight2 = []\n",
    "    weight3 = []\n",
    "    weight4 = []\n",
    "    weight5 = []\n",
    "    weightfc1 = []\n",
    "    weightfc2 = []\n",
    "    \n",
    "    \n",
    "    for e,epoch in enumerate(range(epochs)):\n",
    "        cum_loss = 0\n",
    "       # if e%10 == 0 and e>0:\n",
    "        #    for param_group in optimizer.param_groups:\n",
    "         #       param_group['lr']*=0.1\n",
    "        print('Epoch %d,    lr %f' % (e,optimizer.param_groups[0]['lr']))\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            optimizer.zero_grad()            \n",
    "            scores = model(x)\n",
    "            loss = RMSE(scores, y)\n",
    "            cum_loss += loss.data\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            weight1.append(model.conv1.weight)\n",
    "            weight2.append(model.conv2.weight)\n",
    "            weight3.append(model.conv3.weight)\n",
    "            weight4.append(model.conv4.weight)\n",
    "            weight5.append(model.conv5.weight)\n",
    "            weightfc1.append(model.fc1.weight)\n",
    "            weightfc2.append(model.fc2.weight)\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "                if loader_val:\n",
    "                    check_accuracy_part34(loader_val, model, device)\n",
    "                print()\n",
    "                \n",
    "        hold_loss.append(cum_loss/(t+1))\n",
    "    if loader_val:\n",
    "        check_accuracy_part34(loader_val, model, print_graph=False)\n",
    "    plt.figure()\n",
    "    plt.plot(np.array(hold_loss))\n",
    "    return [weight1,weight2,weight3,weight4,weight5,weightfc1,weightfc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_loader(data = 1, batch_size = 512, valid_size = 0.05, shuffle = True, seed = 42):\n",
    "\n",
    "    trainset = Dataset_Nasa(dataset = data, train = True)\n",
    "    valset = Dataset_Nasa(dataset = data, train = True)\n",
    "    \n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))    \n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = sampler.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    loader_train = DataLoader(trainset, batch_size=batch_size, sampler = train_sampler)\n",
    "    loader_val = DataLoader(valset, batch_size=batch_size, sampler = valid_sampler)\n",
    "    #return loader_train\n",
    "    return (loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainsets = pkl.load(open(\"trainsets.p\",\"rb\"))\n",
    "testsets = pkl.load(open(\"testsets.p\",\"rb\"))\n",
    "\n",
    "train, val = get_train_valid_loader(data = 4)\n",
    "model = CNN()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model,optimizer, train, loader_val = val, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = CNN(activation='relu')\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model2,optimizer2, train, loader_val = val, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = w[0]\n",
    "weight1_1 = [weight1[i][0].data.numpy()[0].squeeze() for i in range(len(weight1))]\n",
    "for i in range(9):\n",
    "    plt.plot([weight1_1[j][i] for j in range(len(weight1_1))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Nasa_sorted(torch.utils.data.Dataset):\n",
    "  def __init__(self, dataset, transform=None, train = True, window = 30):\n",
    "        'Initialization'\n",
    "        df = trainsets[\"FD00\"+str(dataset)] if train else testsets[\"FD00\"+str(dataset)]\n",
    "        df = df.drop(list(set(df[df[\"cycle\"] > 30].index.tolist()) ^ set(range(1,df.index.tolist()[-1]+1))), axis = 0)\n",
    "        if not train:\n",
    "            df_last = [df.loc[i].set_index('cycle').iloc[(-window-1):-1] for i in list(set(df.index.tolist()))]\n",
    "            indices = [df_last[i][\"RUL\"].iloc[-1] for i in range(237)]\n",
    "            permutation = sorted(range(len(indices)), key=lambda k: indices[k])\n",
    "            sorted_testset = [df_last[i] for i in permutation]\n",
    "            df_window = np.vstack([rwindows(a[[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]].values,window) for a in sorted_testset]).reshape((-1,window,15))\n",
    "        else:\n",
    "            df_window = np.vstack([rwindows(df.loc[i][[\"sensor 2\",\"sensor 3\",\"sensor 4\",\"sensor 7\",\"sensor 8\",\"sensor 9\",\"sensor 11\",\"sensor 12\",\"sensor 13\",\"sensor 14\",\"sensor 15\",\"sensor 17\",\"sensor 20\",\"sensor 21\",\"RUL\"]].values,window) for i in list(set(df.index))])\n",
    "        \n",
    "        #remove lines whose cycle time is less than window-size\n",
    "        \n",
    "        \n",
    "        X = df_window[:,:,:14]\n",
    "        Y = df_window[:,29,14]\n",
    "        \n",
    "        \n",
    "        self.X = X.reshape((X.shape[0],1,window,14))\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testset = Dataset_Nasa_sorted(dataset=4, train = False)\n",
    "loader_test = DataLoader(testset, batch_size = 237)\n",
    "\n",
    "check_accuracy_part34(loader_test,model, print_graph = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_act_map(subset,train,model,datapoint):\n",
    "    dataset = Dataset_Nasa(dataset = subset, train = train)\n",
    "    act= {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            act[name] = output.detach().squeeze()\n",
    "        return hook\n",
    "\n",
    "    model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "    model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "    model.conv3.register_forward_hook(get_activation('conv3'))\n",
    "    model.conv4.register_forward_hook(get_activation('conv4'))\n",
    "    model.conv5.register_forward_hook(get_activation('conv5'))\n",
    "    data = torch.from_numpy(dataset[datapoint][0]).view(-1,1,30,14)\n",
    "    output = model(data.to(dtype = torch.float32), torch.tanh)\n",
    "\n",
    "    ax = []\n",
    "    rows = 4\n",
    "    columns = 5\n",
    "    fig = plt.figure(figsize=(13,20))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            ax.append( fig.add_subplot(rows+1, columns, i*columns + j+1) )\n",
    "            ax[-1].set_title(\"layer : \"+str(i+1) + \", act_map : \"+str(j+1)+\"   \")  # set title\n",
    "            plt.imshow(act[\"conv\"+str(i+1)][j], alpha=1)\n",
    "    ax.append( fig.add_subplot(rows+1, columns, (columns)*(rows)+1) )\n",
    "    ax[-1].set_title(\"layer : \"+str(5))  # set title\n",
    "    plt.imshow(act5, alpha=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_act_map(4,True,model,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_act_map(4,True,model,25428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torchviz.make_dot(scores.mean(), params = dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model2.conv1.weight.data.numpy()\n",
    "ax = []\n",
    "fig = plt.figure(figsize=(13,20))\n",
    "for i in range(10):\n",
    "    ax.append( fig.add_subplot(1, 10, i+1 ))\n",
    "    ax[-1].set_title(\"filter :\" + str(i+1))  # set title\n",
    "    plt.imshow(weight[i,0,:,0].reshape((9,1)), alpha=1, cmap = 'gray')\n",
    "    plt.clim(-1,1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight = model2.conv1.weight.data.numpy()\n",
    "plt.imshow(np.transpose(weight[:,0,:,0]))\n",
    "plt.clim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.transpose(weight[:,0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[0,0,:,0].reshape((9,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(weight[0,0,:,0].reshape((9,1)))\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classif(nn.Module):\n",
    "    def __init__(self, window_size =9, dropout_ratio = 0.5, initialization='xavier', hidden_neurons=100, activation = 'tanh'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #self.activ = torch.relu if activation == \"relu\" else torch.tanh\n",
    "        init = nn.init.kaiming_normal_ if initialization==\"kaiming\" else nn.init.xavier_normal_\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        init(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10,10,kernel_size=(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        init(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(10,10,(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh3 = nn.Tanh()\n",
    "        init(self.conv3.weight)\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(10,10,(window_size,1),padding = ((window_size-1)//2,0))\n",
    "        self.tanh4 = nn.Tanh()\n",
    "        init(self.conv4.weight)\n",
    "        nn.init.constant_(self.conv4.bias, 0)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(10,1,(3,1),padding = (1,0))\n",
    "        self.tanh5 = nn.Tanh()\n",
    "        init(self.conv5.weight)\n",
    "        nn.init.constant_(self.conv5.bias, 0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 420, out_features = hidden_neurons)\n",
    "        self.tanh_fc1 = nn.Tanh()\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features = hidden_neurons, out_features = 131)\n",
    "        #self.tanh_fc2 = nn.Tanh()\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        tanh1 = self.tanh1(self.conv1(x))\n",
    "        tanh2 = self.tanh2(self.conv2(tanh1))\n",
    "        tanh3 = self.tanh3(self.conv3(tanh2))\n",
    "        tanh4 = self.tanh4(self.conv4(tanh3))\n",
    "        tanh5 = self.tanh5(self.conv5(tanh4))\n",
    "        dropout1 = self.dropout(tanh5)\n",
    "        x_reshape = dropout1.view(-1,30*14)\n",
    "        tanh6 = self.tanh_fc1(self.fc1(x_reshape))\n",
    "        predict = self.fc2(tanh6)\n",
    "        #print(predict)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classif(model, optimizer,loader_train, loader_val = None, epochs = 1,USE_GPU = True,print_every = 10):\n",
    "    \n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model = model.to(device = device)\n",
    "    hold_loss = []\n",
    "    \n",
    "    dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "    \n",
    "    \n",
    "    for e,epoch in enumerate(range(epochs)):\n",
    "        cum_loss = 0\n",
    "       # if e%10 == 0 and e>0:\n",
    "        #    for param_group in optimizer.param_groups:\n",
    "         #       param_group['lr']*=0.1\n",
    "        print('Epoch %d,    lr %f' % (e,optimizer.param_groups[0]['lr']))\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)            \n",
    "            scores = model(x)\n",
    "            loss = nn.functional.cross_entropy(scores,y)\n",
    "            cum_loss += loss.data\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss))\n",
    "                if loader_val:\n",
    "                    check_accuracy_classif(loader_val, model, device)\n",
    "                print()\n",
    "        hold_loss.append(cum_loss/(t+1))\n",
    "    plt.figure()\n",
    "    plt.plot(np.array(hold_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_classif(loader, model, device = torch.device('cpu'), print_graph = False):\n",
    "    dtype = torch.float32\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct)/num_samples\n",
    "        print('Got : %d / %d correct  (%.2f)' %(num_correct, num_samples, 100*acc))\n",
    "        if print_graph:\n",
    "            plt.scatter(range(len(preds)),np.array(preds))\n",
    "            plt.plot(np.array(y))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainsets = pkl.load(open(\"trainsets.p\",\"rb\"))\n",
    "testsets = pkl.load(open(\"testsets.p\",\"rb\"))\n",
    "\n",
    "train, val = get_train_valid_loader(data = 4)\n",
    "model = CNN_classif()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model_classif(model,optimizer, train, loader_val = val, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset_Nasa(dataset=4, train = True)\n",
    "loader_train = DataLoader(trainset, batch_size = 512)\n",
    "model = CNN_classif()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_model_classif(model,optimizer, loader_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_classif(loader_test,model, print_graph = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
